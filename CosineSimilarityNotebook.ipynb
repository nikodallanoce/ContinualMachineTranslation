{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: pyarrow._fs.FileInfo size changed, may indicate binary incompatibility. Expected 64 from C header, got 88 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: pyarrow._fs.FileSelector size changed, may indicate binary incompatibility. Expected 48 from C header, got 72 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Any\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def create_dataloader(trans_pair_ds: Dataset, input_column: str, fn_kwargs: Dict[str, Any],\n",
    "                      batch_size: int) -> DataLoader:\n",
    "    trans_pair_ds = trans_pair_ds.map(tokenize, batched=True, input_columns=[input_column],\n",
    "                                      fn_kwargs=fn_kwargs)\n",
    "    # trans_pair_ds = trans_pair_ds.remove_columns(column_names=['translation', 'original_text'])\n",
    "    trans_pair_ds = trans_pair_ds.with_format('torch', columns=[\"input_ids\", \"labels\", \"attention_mask\"],\n",
    "                                              output_all_columns=False)\n",
    "\n",
    "    # ids = [e['input_ids'].view(1, -1) for e in iter(trans_pair_ds)]\n",
    "    test_loader = DataLoader(trans_pair_ds, batch_size=batch_size, drop_last=True, pin_memory=False)\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def get_wmt_dataset(lang_pair: str, num_of_rows: int = None) -> Dataset:\n",
    "    wmt14 = \"wmt14\"\n",
    "    split = \"test\"\n",
    "    lang_config = lang_pair.split(\"-\")\n",
    "    assert len(lang_config) == 2\n",
    "    if lang_config[0] == \"en\":\n",
    "        lang_config[0], lang_config[1] = lang_config[1], lang_config[0]\n",
    "    lang_config = \"-\".join(lang_config)\n",
    "    if \"es\" in lang_config:\n",
    "        wmt14 = \"nikodallanoce/wmt14\"\n",
    "        split = \"validation\"\n",
    "    split = split if num_of_rows is None else split + f\"[:{num_of_rows}]\"\n",
    "    ds = load_dataset(wmt14, lang_config,\n",
    "                      cache_dir=\"/data/n.dallanoce/wmt14\",\n",
    "                      split=split,\n",
    "                      verification_mode='no_checks')\n",
    "    return ds\n",
    "\n",
    "\n",
    "def tokenize(examples: List[Dict[str, str]], **kwargs):\n",
    "    tokenizer = kwargs['tokenizer']\n",
    "    src_lang: str = kwargs['lang1']\n",
    "    tgt_lang: str = kwargs['lang2']\n",
    "    if \"task\" in kwargs:\n",
    "        task: str = kwargs['task']\n",
    "        batch_src: List[str] = [task + e[src_lang] for e in examples]\n",
    "    else:\n",
    "        batch_src: List[str] = [e[src_lang] for e in examples]\n",
    "    batch_tgt: List[str] = [e[tgt_lang] for e in examples]\n",
    "    # tokenize the batch of sentences\n",
    "    outputs = tokenizer(batch_src, text_target=batch_tgt, return_special_tokens_mask=False,\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=128, padding='max_length',\n",
    "                        return_attention_mask=True, return_tensors='pt')\n",
    "    # labels = tokenizer(batch_tgt, truncation=False)\n",
    "    # batch_tgt = tokenizer.batch_decode(labels['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "    return {'input_ids': outputs['input_ids'], 'labels': outputs['labels'], 'attention_mask': outputs['attention_mask']}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:27:23.916852700Z",
     "start_time": "2023-09-21T16:27:23.042260600Z"
    }
   },
   "id": "2b4619446770d6cd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "from CosineSim import CosineSim\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:27:24.528261900Z",
     "start_time": "2023-09-21T16:27:23.915853500Z"
    }
   },
   "id": "d6a4ba70811e1c67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# mBART"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d1a4ef1b02da9a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (/data/n.dallanoce/wmt14/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Loading cached processed dataset at /data/n.dallanoce/wmt14/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-3bd4b67a2a65c9e0.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartConfig, MBartForConditionalGeneration\n",
    "from utilities.models import get_all_mbart_models\n",
    "\n",
    "tok_mbart = AutoTokenizer.from_pretrained(\"nikodallanoce/mbart-cc4-vanilla-32k-5\", src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\n",
    "\n",
    "fn_kwargs_mbart = {'tokenizer': tok_mbart, 'lang1': \"en\", 'lang2': \"fr\"}\n",
    "wmt14_ds_mbart = get_wmt_dataset(fn_kwargs_mbart['lang1'] + \"-\" + fn_kwargs_mbart['lang2'], num_of_rows=512)\n",
    "dataloader_mbart = create_dataloader(wmt14_ds_mbart, \"translation\", fn_kwargs_mbart, 32)\n",
    "mbart_config = MBartConfig(encoder_layers=6, decoder_layers=6,\n",
    "                           encoder_ffn_dim=2048, decoder_ffn_dim=2048,\n",
    "                           encoder_attention_heads=8, decoder_attention_heads=8,\n",
    "                           d_model=512, max_length=128, vocab_size=tok_mbart.vocab_size, dropout=0.1)\n",
    "rnd_mbart = MBartForConditionalGeneration(mbart_config)\n",
    "\n",
    "mbart_models = get_all_mbart_models()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:27:46.953122200Z",
     "start_time": "2023-09-21T16:27:24.528261900Z"
    }
   },
   "id": "877cdaf385a533a9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cs = CosineSim(mbart_models[\"M1\"], mbart_models[\"M2_replay\"])\n",
    "sim = cs.compute_enc_hidd_states(dataloader_mbart)\n",
    "print(round(sim, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T15:29:18.148374800Z",
     "start_time": "2023-09-21T15:29:06.061383900Z"
    }
   },
   "id": "ce65f2c9af8a97a8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]Found cached dataset wmt14 (/data/n.dallanoce/wmt14/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Loading cached processed dataset at /data/n.dallanoce/wmt14/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4/cache-3bd4b67a2a65c9e0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between M1 and M2 is 0.587\n",
      "Similarity between M1 and M2_de_only is 0.0131\n",
      "Similarity between M1 and M2_replay is 0.9104\n",
      "Similarity between M1 and M3 is 0.4945\n",
      "Similarity between M1 and M3_replay is 0.8724\n",
      "Similarity between M1 and M1F1 is 0.6012\n",
      "Similarity between M1 and M2F1 is 0.5645\n",
      "Similarity between M1 and M2F1_replay is 0.5939\n",
      "Similarity between M1 and M2F2 is 0.489\n",
      "Similarity between M1 and MF2_ft_only is 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 15.75 GiB total capacity; 12.12 GiB already allocated; 1.90 GiB free; 12.75 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m mj_name, model_j \u001B[38;5;241m=\u001B[39m model_lst[j]\n\u001B[1;32m     16\u001B[0m cs \u001B[38;5;241m=\u001B[39m CosineSim(model_i, model_j)\n\u001B[0;32m---> 17\u001B[0m sim \u001B[38;5;241m=\u001B[39m \u001B[43mcs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_enc_hidd_states\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader_mbart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSimilarity between \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmi_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmj_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(sim,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m4\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PyCharm/pretraining/CosineSim.py:36\u001B[0m, in \u001B[0;36mCosineSim.compute_enc_hidd_states\u001B[0;34m(self, dataloader, show_tqdm)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch[e], torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m     34\u001B[0m         batch[e] \u001B[38;5;241m=\u001B[39m batch[e]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 36\u001B[0m seq2seq_out_m1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m seq2seq_out_m2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel2(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch)\n\u001B[1;32m     39\u001B[0m last_hidd_state_m1 \u001B[38;5;241m=\u001B[39m seq2seq_out_m1[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoder_last_hidden_state\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mT\n",
      "File \u001B[0;32m~/anaconda3/envs/deeptorch/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 727\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    729\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    730\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    731\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeptorch/lib/python3.8/site-packages/transformers/models/mbart/modeling_mbart.py:1368\u001B[0m, in \u001B[0;36mMBartForConditionalGeneration.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1349\u001B[0m         decoder_input_ids \u001B[38;5;241m=\u001B[39m shift_tokens_right(labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id)\n\u001B[1;32m   1351\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m   1352\u001B[0m     input_ids,\n\u001B[1;32m   1353\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1366\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1367\u001B[0m )\n\u001B[0;32m-> 1368\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_logits_bias\u001B[49m\n\u001B[1;32m   1370\u001B[0m masked_lm_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 15.75 GiB total capacity; 12.12 GiB already allocated; 1.90 GiB free; 12.75 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "langs = [\"en\", \"fr\", \"de\", \"es\"]\n",
    "model_lst = [(k, v) for k, v in mbart_models.items()]\n",
    "\n",
    "for lang in tqdm(langs):\n",
    "    fn_kwargs_mbart = {'tokenizer': tok_mbart, 'lang1': lang, 'lang2': \"fr\"}\n",
    "    if lang != \"en\":\n",
    "        fn_kwargs_mbart['lang2'] = \"en\"\n",
    "    wmt14_ds_mbart = get_wmt_dataset(fn_kwargs_mbart['lang1'] + \"-\" + fn_kwargs_mbart['lang2'], num_of_rows=512)\n",
    "    dataloader_mbart = create_dataloader(wmt14_ds_mbart, \"translation\", fn_kwargs_mbart, 128)\n",
    "    for i in range(len(model_lst) - 1):\n",
    "        mi_name, model_i = model_lst[i]\n",
    "        for j in range(i + 1, len(model_lst)):\n",
    "            mj_name, model_j = model_lst[j]\n",
    "            cs = CosineSim(model_i, model_j)\n",
    "            sim = cs.compute_enc_hidd_states(dataloader_mbart, show_tqdm=False)\n",
    "            print(f\"Similarity between {mi_name} and {mj_name} is {round(sim, 4)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:29:55.664864900Z",
     "start_time": "2023-09-21T16:29:05.079835200Z"
    }
   },
   "id": "5db9ec95d599aebf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sim = cs.compute_logits(dataloader_mbart)\n",
    "print(round(sim, 4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d50c12fdd05b401"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# mT6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f1ff9085cade79"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (/data/n.dallanoce/wmt14/nikodallanoce___wmt14/es-en/1.0.0/87db7d5f83bc44f038b67325c372011ddb3cb63ec2bb219b5736426178356f0a)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/512 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49836e3f9a554aa7a2e4ae3f83501c30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Config\n",
    "from utilities.models import get_all_mt6_models\n",
    "\n",
    "tok_mt6 = AutoTokenizer.from_pretrained(\"nikodallanoce/mt5-cc4-vanilla-32k-5\")\n",
    "rnd_mt6 = MT5ForConditionalGeneration(\n",
    "    MT5Config(num_layers=6, d_model=512, num_heads=8, d_ff=2048, vocab_size=len(tok_mt6), max_length=128,\n",
    "              tie_word_embeddings=True))\n",
    "fn_kwargs_mt6 = {'tokenizer': tok_mt6, 'lang1': \"en\", 'lang2': \"es\"}\n",
    "wmt14_ds_mt6 = get_wmt_dataset(fn_kwargs_mt6['lang1'] + \"-\" + fn_kwargs_mt6['lang2'], num_of_rows=512)\n",
    "dataloader_mt6 = create_dataloader(wmt14_ds_mt6, \"translation\", fn_kwargs_mt6, 32)\n",
    "mt6_models = get_all_mt6_models()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T09:05:35.733948700Z",
     "start_time": "2023-09-21T09:05:12.702104300Z"
    }
   },
   "id": "2dbb1616933a745"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:06<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cs = CosineSim(mbart_models[\"M1\"], mbart_models[\"M2_replay\"])\n",
    "sim = cs.compute_enc_hidd_states(dataloader_mt6)\n",
    "print(round(sim, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T09:05:55.597234100Z",
     "start_time": "2023-09-21T09:05:48.672706300Z"
    }
   },
   "id": "5255061525c9de3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
