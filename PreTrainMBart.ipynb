{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:16:00.057407Z",
     "start_time": "2023-11-28T16:15:57.120345300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers.utils import is_torch_fx_proxy\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import MBartModel, MBartTokenizer, MBartConfig, MBartForConditionalGeneration\n",
    "from datasets import concatenate_datasets, load_from_disk\n",
    "from torch.optim import *\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024 ** 2\n",
    "    return size_all_mb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:16:00.068420500Z",
     "start_time": "2023-11-28T16:16:00.056426200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.07921600341797\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#pre_train_ds = load_dataset(\"text\", data_files={\"train\": [\"/data/n.dallanoce/cc100/en.txt\"]},\n",
    "#                            cache_dir=\"/data/n.dallanoce/cc100/hugg_en\", split='train', ignore_verifications=True)\n",
    "tok_name = \"thesistranslation/mbart_en_fr\"\n",
    "tok_en = AutoTokenizer.from_pretrained(tok_name, src_lang=\"en_XX\")\n",
    "cuda_dev = \"cpu\"\n",
    "#model = model.to(cuda_dev)\n",
    "model: MBartForConditionalGeneration = MBartForConditionalGeneration.from_pretrained(\n",
    "    \"/data/n.dallanoce/weights/mt5_ft_en-fr_M1F1_t5_mbart/checkpoint-5000\").to(cuda_dev)\n",
    "model.train(False)\n",
    "print(model_size(model))\n",
    "\n",
    "#dataset_loaded = load_from_disk(\"europarl_eng_tokenized\")\n",
    "#pre_train_load = DataLoader(pre_train_ds, batch_size=8, drop_last=True, shuffle=False, pin_memory=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T09:18:09.811831300Z",
     "start_time": "2023-07-03T09:18:07.738301800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MBartForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): Embedding(32030, 512, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32030, bias=False)\n",
      "), MBartModel(\n",
      "  (shared): Embedding(32030, 512, padding_idx=1)\n",
      "  (encoder): MBartEncoder(\n",
      "    (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "    (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): MBartEncoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): MBartDecoder(\n",
      "    (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "    (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): MBartDecoderLayer(\n",
      "        (self_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MBartAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "), Embedding(32030, 512, padding_idx=1), MBartEncoder(\n",
      "  (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "  (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): MBartEncoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartLearnedPositionalEmbedding(1026, 512), ModuleList(\n",
      "  (0): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): MBartEncoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartEncoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), GELUActivation(), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoder(\n",
      "  (embed_tokens): Embedding(32030, 512, padding_idx=1)\n",
      "  (embed_positions): MBartLearnedPositionalEmbedding(1026, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): MBartDecoderLayer(\n",
      "      (self_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MBartAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartLearnedPositionalEmbedding(1026, 512), ModuleList(\n",
      "  (0): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): MBartDecoderLayer(\n",
      "    (self_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): MBartAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartDecoderLayer(\n",
      "  (self_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MBartAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), GELUActivation(), LayerNorm((512,), eps=1e-05, elementwise_affine=True), MBartAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), Linear(in_features=512, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=2048, bias=True), Linear(in_features=2048, out_features=512, bias=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), LayerNorm((512,), eps=1e-05, elementwise_affine=True), Linear(in_features=512, out_features=32030, bias=False)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.modules()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T09:38:59.663140Z",
     "start_time": "2023-07-03T09:38:59.630971200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.model.encoder.layers[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T16:06:17.604616500Z",
     "start_time": "2023-07-04T16:06:17.192275100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([512, 512]), torch.Size([512])]\n",
      "[Parameter containing:\n",
      "tensor([[-0.0552, -0.0768,  0.5730,  ...,  0.0457,  0.3106,  0.0737],\n",
      "        [-0.2644, -0.0690,  0.0978,  ...,  0.2587, -0.2191,  0.2115],\n",
      "        [ 0.0304, -0.3047,  0.3530,  ...,  0.1265,  0.2673, -0.0974],\n",
      "        ...,\n",
      "        [-0.0083, -0.0276,  0.0968,  ...,  0.3071, -0.0715, -0.0855],\n",
      "        [-0.5523,  0.2412, -0.0553,  ..., -0.1232,  0.0274,  0.1091],\n",
      "        [ 0.1764, -0.0348, -0.1450,  ..., -0.2204,  0.1946,  0.0720]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 1.1361e-02, -9.2545e-02,  1.2006e-01,  9.9594e-03, -9.1281e-02,\n",
      "        -2.9175e-02,  1.0413e-01,  2.1937e-01, -1.2781e-01,  1.3878e-02,\n",
      "        -4.9402e-03,  1.7313e-01,  7.9461e-05,  1.8383e-01,  9.9675e-02,\n",
      "        -1.4741e-02, -2.9353e-01,  6.7065e-02,  1.8205e-01,  2.6332e-02,\n",
      "        -9.2180e-02,  8.1779e-02, -2.9540e-02,  1.1995e-01,  1.8370e-01,\n",
      "         9.8391e-02, -1.3431e-01, -1.1188e-01,  1.8217e-01,  1.0096e-01,\n",
      "         1.0092e-01, -9.8337e-02, -1.4348e-01, -1.7365e-01,  1.7517e-01,\n",
      "         1.6521e-02,  5.4720e-02,  1.0443e-01,  1.9672e-01,  1.1490e-01,\n",
      "         2.6782e-02,  3.1570e-01,  2.3912e-02,  7.8657e-02, -4.2486e-02,\n",
      "         1.1878e-01, -2.5669e-01, -2.2808e-03,  1.4535e-01, -1.7135e-01,\n",
      "        -6.4779e-02,  7.9791e-02, -2.2592e-01, -7.1617e-02, -9.4197e-02,\n",
      "        -1.4966e-01, -2.2263e-01, -2.0152e-01, -3.2577e-01,  8.5693e-02,\n",
      "        -8.9310e-02, -8.6242e-02, -6.1703e-02,  1.1168e-01, -2.5640e-02,\n",
      "        -7.2604e-02,  1.1821e-02, -1.6683e-02,  2.8810e-02, -1.5081e-02,\n",
      "        -2.8607e-02,  9.7101e-03, -3.7945e-02, -2.3519e-02,  1.5607e-02,\n",
      "         1.9215e-02, -9.4238e-05,  2.9974e-02,  1.9978e-02,  4.6404e-03,\n",
      "        -3.4878e-04, -2.8176e-03, -1.2087e-03,  9.5505e-03, -1.6511e-02,\n",
      "        -5.3677e-03, -3.3906e-02, -5.5778e-02, -3.4659e-02,  1.0104e-02,\n",
      "         2.8980e-03, -3.7752e-02,  1.5840e-03, -1.6240e-02,  1.4154e-02,\n",
      "        -2.0952e-02, -5.7912e-03,  1.9856e-03, -2.1933e-02, -3.5607e-02,\n",
      "         1.1431e-02, -1.1363e-02, -2.3670e-03, -2.2845e-02, -1.2038e-02,\n",
      "        -2.7553e-02,  5.5794e-03,  4.8532e-02,  1.9656e-03,  2.6497e-02,\n",
      "        -1.3062e-02,  1.5352e-02,  1.2378e-02,  4.6147e-02,  2.5941e-02,\n",
      "         9.6186e-03,  6.2215e-03, -2.4025e-02,  1.0956e-02, -1.4470e-02,\n",
      "         3.0530e-02,  2.9346e-02, -7.2296e-03, -7.2913e-04,  1.3291e-02,\n",
      "         4.0711e-02, -2.5619e-02, -2.3610e-03,  1.0127e-02, -2.9933e-02,\n",
      "        -6.0792e-03, -2.8446e-02,  2.2624e-02, -3.4613e-02, -2.4745e-02,\n",
      "         4.2641e-02,  4.1816e-02, -2.0527e-02,  7.6003e-02, -2.5616e-02,\n",
      "         3.6780e-02,  7.3223e-02,  3.2092e-02, -5.0365e-03,  3.1765e-02,\n",
      "        -2.9794e-03, -2.7808e-02,  5.5614e-02,  4.8631e-03, -1.0056e-02,\n",
      "         1.8855e-02, -8.9068e-03, -2.0389e-02,  3.1818e-02, -4.5497e-02,\n",
      "         3.4205e-02, -2.0343e-03,  1.3898e-02,  6.8674e-02,  7.6353e-03,\n",
      "        -2.6516e-02,  1.3210e-02, -1.5185e-02,  2.0782e-02, -3.0040e-02,\n",
      "         8.0572e-03,  3.3552e-02, -2.2693e-03, -1.2399e-02,  2.1191e-02,\n",
      "         3.3792e-02,  3.0420e-02,  2.3014e-02,  1.6958e-02, -6.9090e-03,\n",
      "        -4.9267e-02,  4.1984e-02, -4.1764e-02, -4.5587e-02, -1.3581e-02,\n",
      "        -1.1234e-02, -3.4452e-02,  8.4637e-03, -6.4174e-02,  4.4350e-02,\n",
      "         4.6739e-02, -2.4560e-02,  1.1005e-02, -2.8788e-02, -6.2809e-04,\n",
      "         5.1656e-03, -1.2914e-02,  2.7750e-02, -8.3489e-02,  2.1385e-02,\n",
      "        -8.3709e-02, -2.7859e-02,  3.2841e-02,  1.4944e-02,  6.6429e-02,\n",
      "        -3.8137e-02,  3.5868e-02, -8.9256e-04, -9.1316e-02, -4.5344e-02,\n",
      "        -5.3102e-02,  3.5201e-02, -9.7711e-02,  2.4165e-02,  3.6946e-02,\n",
      "        -1.2640e-01, -7.4283e-02, -1.2695e-02,  5.9302e-02,  1.7900e-02,\n",
      "         1.1416e-01,  3.2410e-02,  1.1186e-01,  3.5906e-02,  2.1337e-02,\n",
      "        -8.6098e-02, -4.0401e-02,  1.3748e-01, -9.0413e-02,  8.4995e-02,\n",
      "        -5.8486e-02,  1.2442e-01,  1.2264e-02,  6.4865e-02,  2.2151e-02,\n",
      "         3.0234e-02,  3.1174e-02,  2.3045e-02, -5.5579e-02,  2.5331e-02,\n",
      "        -1.3491e-01, -7.2520e-02, -4.6886e-02, -9.8424e-02,  3.1086e-02,\n",
      "         5.3310e-03, -1.1243e-02,  2.0939e-02,  8.8738e-02,  6.5681e-02,\n",
      "        -1.4979e-02, -3.3822e-02, -3.0299e-02, -3.6238e-04,  1.1752e-01,\n",
      "        -4.7171e-02, -7.0664e-03,  1.3517e-02, -1.5620e-01, -8.1561e-02,\n",
      "         7.4812e-02,  1.8265e-01, -1.1477e-01, -2.2296e-01,  1.1365e-01,\n",
      "        -9.6737e-02, -2.4511e-01, -4.2634e-02, -1.5966e-02, -6.6743e-02,\n",
      "        -1.6443e-01,  1.2898e-02,  2.3853e-02, -8.3825e-02,  2.8411e-04,\n",
      "         4.7998e-02,  2.2283e-02,  1.7698e-01,  3.5249e-02, -4.6358e-02,\n",
      "        -1.2757e-01, -1.5385e-01,  2.9468e-02,  2.0362e-01, -5.6240e-02,\n",
      "        -9.3437e-02, -7.1844e-03, -4.5913e-02,  1.0529e-01, -7.5900e-02,\n",
      "        -3.4540e-03,  2.2585e-01,  2.0807e-01,  1.7840e-01, -3.1637e-02,\n",
      "        -9.5769e-02, -3.4168e-02, -2.8011e-01, -1.3000e-01,  4.0104e-02,\n",
      "        -7.0251e-02, -8.2629e-03,  6.8511e-02,  6.9810e-02,  1.2320e-01,\n",
      "        -1.0865e-01,  6.4683e-02,  4.2913e-01,  1.2419e-02, -9.9511e-02,\n",
      "         9.6406e-02, -7.3559e-02,  8.2254e-02, -1.7399e-01,  8.7560e-02,\n",
      "        -8.6651e-02, -3.1282e-02, -4.4098e-01,  2.4961e-01,  3.3723e-02,\n",
      "        -2.2032e-01, -1.7486e-01,  1.6684e-01,  1.5690e-01,  6.0677e-02,\n",
      "        -3.7985e-02,  1.2032e-03, -4.0744e-03,  1.1372e-02,  1.9058e-02,\n",
      "        -2.5240e-02,  1.8069e-02,  1.0673e-02, -1.4051e-02,  4.8273e-02,\n",
      "        -4.5393e-02, -8.5538e-04,  1.0837e-02,  3.6129e-02, -1.9034e-02,\n",
      "         2.1630e-02,  4.7786e-03,  4.4669e-02,  3.8274e-03, -4.2220e-03,\n",
      "        -6.6278e-02,  1.5549e-02, -4.1933e-02, -1.4174e-03,  5.6399e-03,\n",
      "        -3.6883e-02,  6.7681e-03,  7.1817e-03, -9.2428e-03, -9.8775e-03,\n",
      "        -3.6083e-02,  6.7593e-04,  2.8209e-03, -2.0470e-02, -4.1539e-02,\n",
      "        -3.8867e-02,  3.5606e-02, -2.3257e-02, -1.7724e-02, -1.8965e-02,\n",
      "        -1.6178e-02,  6.3308e-02,  1.1678e-01,  1.5778e-02, -2.7812e-02,\n",
      "        -2.6153e-02, -1.6019e-02,  5.3218e-02, -1.0586e-03, -2.9204e-03,\n",
      "         4.6875e-03, -9.8699e-02,  1.1054e-02,  9.8126e-02,  9.4007e-03,\n",
      "        -2.8171e-03, -9.5715e-03, -1.4772e-02, -5.3199e-02, -5.3673e-02,\n",
      "        -4.7876e-03,  4.3921e-02,  3.1543e-03, -3.3983e-02,  1.2105e-02,\n",
      "        -2.6952e-02,  5.2211e-02,  4.7661e-03,  1.2474e-02,  4.2619e-02,\n",
      "        -2.9756e-02,  3.6913e-02, -2.4197e-02, -8.2596e-02,  4.9099e-02,\n",
      "         1.8529e-02, -5.5049e-02,  2.4897e-02,  8.2251e-03, -4.5805e-02,\n",
      "         1.9883e-02, -9.8247e-02,  9.3685e-03, -2.7993e-03,  2.5213e-03,\n",
      "         2.1302e-03, -1.0014e-01, -6.1900e-02, -9.8399e-03,  4.2604e-02,\n",
      "         2.0454e-02,  1.0706e-01, -4.4870e-02,  3.6202e-02,  7.2898e-02,\n",
      "         1.7762e-02,  2.5785e-02, -1.4679e-02,  2.6470e-03, -4.9370e-02,\n",
      "        -6.9373e-02, -1.6235e-02, -1.1951e-01, -3.4791e-02, -4.6194e-02,\n",
      "        -2.7690e-02, -4.2274e-02, -1.6565e-02,  2.8377e-02,  5.0707e-02,\n",
      "        -4.6360e-02, -1.2306e-02, -1.6753e-02,  8.0547e-03,  6.7092e-02,\n",
      "        -4.9778e-02,  4.6906e-02, -9.8189e-03, -9.4388e-02, -5.1859e-02,\n",
      "         1.5459e-02, -5.4489e-02, -2.1077e-02,  6.1128e-02, -1.7691e-03,\n",
      "        -2.9820e-02,  1.1638e-02, -1.2031e-02, -1.0560e-02, -1.6009e-02,\n",
      "        -4.4074e-02, -1.2589e-02,  3.8874e-02, -4.5717e-03,  3.1713e-02,\n",
      "         2.2667e-02, -1.2464e-02, -3.7005e-02, -1.4559e-02,  5.3449e-02,\n",
      "         4.2182e-02,  1.7754e-02, -2.0166e-03,  5.7199e-02,  6.8175e-02,\n",
      "        -5.8566e-02,  1.0035e-03,  3.4012e-02,  2.0183e-02,  9.4707e-02,\n",
      "        -1.4904e-02, -1.5391e-02,  1.6799e-02,  2.5048e-02, -2.2987e-02,\n",
      "         1.1694e-02,  2.6392e-03, -4.3147e-03, -3.8278e-02,  4.3470e-02,\n",
      "        -1.9765e-02, -1.3060e-02,  3.5805e-03,  2.1808e-02, -1.8694e-02,\n",
      "         3.0204e-03, -3.7391e-03,  2.6924e-02, -1.6849e-03, -9.1295e-03,\n",
      "        -9.1562e-02, -3.6152e-02, -3.9887e-02,  2.5349e-02,  9.9788e-05,\n",
      "         2.1959e-02,  7.6715e-03, -4.0798e-02,  2.4728e-02,  8.6285e-03,\n",
      "         8.6505e-02, -4.6811e-02,  1.0139e-02, -1.3381e-02, -1.1340e-02,\n",
      "        -3.1963e-02, -4.4121e-02, -4.6743e-03, -1.7515e-02,  8.2107e-03,\n",
      "        -7.0927e-02,  2.0286e-02], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "param_lst = list(model.model.get_submodule(\"encoder.layers.0.self_attn.k_proj\").parameters())\n",
    "print([e.shape for e in param_lst])\n",
    "print(param_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T09:35:55.772884200Z",
     "start_time": "2023-07-03T09:35:55.743784700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.nn.functional.gelu(param_lst[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T09:32:22.452741800Z",
     "start_time": "2023-07-03T09:32:22.445727500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['fr_XX Que faites-vous pour la session?</s>']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"What are you doing for the session?</s> How are you?\"\n",
    "out = tok_en(sent, add_special_tokens=True, return_tensors=\"pt\")\n",
    "gen = model.generate(out['input_ids'].to(cuda_dev), num_beams=5, max_length=128,\n",
    "                     decoder_start_token_id=tok_en.convert_tokens_to_ids(\"fr_XX\"))\n",
    "tok_en.batch_decode(gen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T10:27:49.111370900Z",
     "start_time": "2023-06-29T10:27:48.246019300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.1152, -0.1643,  0.1987,  ...,  0.1372, -0.0425, -0.2086],\n        [ 0.1267, -0.1585,  0.1778,  ...,  0.1125, -0.0720, -0.2186],\n        [-0.1357, -0.1363,  0.0593,  ...,  0.0152, -0.0454, -0.2686],\n        ...,\n        [ 0.1230, -0.1556,  0.1748,  ...,  0.1078, -0.0689, -0.2171],\n        [ 0.1261, -0.1566,  0.1794,  ...,  0.1139, -0.0656, -0.2184],\n        [ 0.1245, -0.1576,  0.1732,  ...,  0.1074, -0.0787, -0.2140]],\n       requires_grad=True)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.shared.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T09:30:55.581152Z",
     "start_time": "2023-06-01T09:30:55.566296200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.1152, -0.1643,  0.1987,  ...,  0.1372, -0.0425, -0.2086],\n        [ 0.1267, -0.1585,  0.1778,  ...,  0.1125, -0.0720, -0.2186],\n        [-0.1357, -0.1363,  0.0593,  ...,  0.0152, -0.0454, -0.2686],\n        ...,\n        [ 0.1230, -0.1556,  0.1748,  ...,  0.1078, -0.0689, -0.2171],\n        [ 0.1261, -0.1566,  0.1794,  ...,  0.1139, -0.0656, -0.2184],\n        [ 0.1245, -0.1576,  0.1732,  ...,  0.1074, -0.0787, -0.2140]],\n       requires_grad=True)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T09:30:58.839984200Z",
     "start_time": "2023-06-01T09:30:58.833444900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "61592576"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T09:34:10.343618300Z",
     "start_time": "2023-06-01T09:34:10.303709100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_tot = 0\n",
    "for param in model.parameters():\n",
    "    param_tot += param.nelement()\n",
    "print(param_tot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-31T15:14:15.030588Z",
     "end_time": "2023-03-31T15:14:15.034130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = \"Travelling alone can be a daunting prospect, not least to attend a wedding alone. The original plan had been to go with another neighbour, but she slipped a disc in her back and was unable to fly.\"\n",
    "test_ids = tok_en([sentence], add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(test_ids.to(cuda_dev)).logits\n",
    "masked_index = (test_ids[0] == tok_en.mask_token_id)  #.nonzero().item()\n",
    "masked_index = torch.nonzero(masked_index).item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "tok_en.decode(predictions).split()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_english_phrase = \"Travelling alone can be a daunting prospect, not least to attend a wedding alone. The original plan had been to go with another neighbour, but she slipped a disc in her back and was unable to fly.\"\n",
    "batch = tok_en(example_english_phrase, return_tensors=\"pt\").to(cuda_dev)\n",
    "print(batch[\"input_ids\"])\n",
    "generated_ids = model.generate(batch[\"input_ids\"], max_new_tokens=128,\n",
    "                               pad_token_id=tok_en.pad_token_id,\n",
    "                               eos_token_id=tok_en.eos_token_id)\n",
    "print(tok_en.batch_decode(generated_ids, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_english_phrase = \"Loving my <mask> almost due can't wait to see my baby boy.\"\n",
    "batch = tok_en(example_english_phrase, return_tensors=\"pt\").to(cuda_dev)\n",
    "print(batch[\"input_ids\"])\n",
    "generated_ids = model.generate(batch[\"input_ids\"], max_new_tokens=128,\n",
    "                               pad_token_id=tok_en.pad_token_id,\n",
    "                               eos_token_id=tok_en.eos_token_id,\n",
    "                               decoder_start_token_id=tok_en.lang_code_to_id[tok_en.src_lang])\n",
    "print(tok_en.batch_decode(generated_ids, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc100_fr = load_dataset(\"cc100\", lang=\"fr\",\n",
    "                        cache_dir=\"/data/n.dallanoce/cc100/huggingface\",\n",
    "                        split=f\"train[:1024]\",\n",
    "                        verification_mode='no_checks')\n",
    "cc100_fr[5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tok_fr = AutoTokenizer.from_pretrained(tok_name, src_lang=\"fr_XX\")\n",
    "sent = \"Je penses et rflchit tout seul sur des sujets n'ayant rien  voir avec une situation. Exemple, quand j'attends mes soeurs et ma mre devant un magasin, je rflchit  la cration du monde.\"\n",
    "batch = tok_fr(sent, return_tensors=\"pt\").to(cuda_dev)\n",
    "print(batch[\"input_ids\"])\n",
    "generated_ids = model.generate(batch[\"input_ids\"], max_new_tokens=128,\n",
    "                               pad_token_id=tok_fr.pad_token_id,\n",
    "                               eos_token_id=tok_fr.eos_token_id,\n",
    "                               decoder_start_token_id=tok_fr.lang_code_to_id[tok_fr.src_lang])\n",
    "print(tok_en.batch_decode(generated_ids, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tok_fr.lang_code_to_id[tok_fr.src_lang]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
